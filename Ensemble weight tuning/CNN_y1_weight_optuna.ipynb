{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1244979,"status":"ok","timestamp":1700588687477,"user":{"displayName":"蔡承哲","userId":"08856972309667959252"},"user_tz":-480},"id":"DIgfc-_5neIq","outputId":"cbc49d83-54b1-430f-9794-41c813ed0105"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x7ed18c76ca00>"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras import layers\n","from tensorflow.keras.models import Sequential\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","# from tensorflow.keras.layers import LSTM,Dense,Conv1D,MaxPooling1D,TimeDistributed,Flatten,Bidirectional,GlobalAveragePooling1D\n","# prdiction_y = str(input('please input the predicion value :'))\n","\n","prdiction_y = 'y1'\n","window_size = 30\n","kernel_size = 3\n","pool_size = 2\n","epoch = 10\n","batch_size = 50\n","learning_rate = 0.0001\n","\n","A1 = pd.read_csv('/content/drive/MyDrive/2023 IMBD/1.csv') # '/TOPIC/projectA/train1/anomaly_train1.csv'\n","A2 = pd.read_csv('/content/drive/MyDrive/2023 IMBD/2.csv')\n","A3 = pd.read_csv('/content/drive/MyDrive/2023 IMBD/3.csv')\n","A4 = pd.read_csv('/content/drive/MyDrive/2023 IMBD/4.csv')\n","A5 = pd.read_csv('/content/drive/MyDrive/2023 IMBD/5.csv')\n","\n","def process_input_data_with_targets(input_data, target_data , window_length , shift ):\n","  num_batches = np.int(np.floor((len(input_data) - window_length)/shift)) + 1 # 共有多少組小資料\n","  num_features = input_data.shape[1] # 每筆資料的特徵數\n","  output = np.repeat(np.nan, repeats = num_batches * window_length * num_features)\n","  output_data = output.reshape(num_batches, window_length , num_features)\n","\n","  if target_data is None: #測試資料無target data\n","    for batch in range(num_batches):\n","      output_data[batch,:,:] = input_data[(0+shift*batch):(0+shift*batch+window_length),:]\n","    return output_data\n","  else : #處理訓練資料的target data\n","    output_targets = np.repeat(np.nan, repeats = num_batches) #target rul\n","    for batch in range(num_batches):\n","      window_start = shift * batch\n","      window_end = window_start + window_length\n","\n","      output_data[batch,:,:] = input_data[window_start:window_end,:]\n","      output_targets[batch] = target_data[window_end-1]\n","    return output_data, output_targets\n","\n","train_x_1 = A1.drop( ['y1','y2','y3'] ,axis= 1)\n","train_x_2 = A2.drop( ['y1','y2','y3'] ,axis= 1)\n","train_x_3 = A3.drop( ['y1','y2','y3'] ,axis= 1)\n","train_x_4 = A4.drop( ['y1','y2','y3'] ,axis= 1)\n","train_x_5 = A5.drop( ['y1','y2','y3'] ,axis= 1)\n","\n","# 正規化數據\n","scaled_train_data_1 = MinMaxScaler().fit_transform(train_x_1)\n","scaled_train_data_2 = MinMaxScaler().fit_transform(train_x_2)\n","scaled_train_data_3 = MinMaxScaler().fit_transform(train_x_3)\n","scaled_train_data_4 = MinMaxScaler().fit_transform(train_x_4)\n","scaled_train_data_5 = MinMaxScaler().fit_transform(train_x_5)\n","\n","train_y1_1 = A1[prdiction_y]\n","train_y1_2 = A2[prdiction_y]\n","train_y1_3 = A3[prdiction_y]\n","train_y1_4 = A4[prdiction_y]\n","train_y1_5 = A5[prdiction_y]\n","\n","train_output_data_1, train_output_targets_1 = process_input_data_with_targets(scaled_train_data_1, train_y1_1.values ,window_length= window_size, shift = 1)\n","train_output_data_2, train_output_targets_2 = process_input_data_with_targets(scaled_train_data_2, train_y1_2.values ,window_length= window_size, shift = 1)\n","train_output_data_3, train_output_targets_3 = process_input_data_with_targets(scaled_train_data_3, train_y1_3.values ,window_length= window_size, shift = 1)\n","train_output_data_4, train_output_targets_4 = process_input_data_with_targets(scaled_train_data_4, train_y1_4.values ,window_length= window_size, shift = 1)\n","train_output_data_5, train_output_targets_5 = process_input_data_with_targets(scaled_train_data_5, train_y1_5.values ,window_length= window_size, shift = 1)\n","\n","model_1 = Sequential([\n","  layers.Conv1D(128, kernel_size = kernel_size, padding = \"same\", activation=\"relu\", input_shape = (window_size,5)),\n","  layers.MaxPooling1D(pool_size= pool_size, padding='same'),\n","  layers.Conv1D(128,kernel_size = kernel_size, padding = \"same\", activation=\"relu\"),\n","  layers.MaxPooling1D(pool_size= pool_size, padding='same'),\n","  layers.GlobalAveragePooling1D(),\n","  # layers.Dropout(0.5),\n","  layers.Dense(96, activation = \"relu\"),\n","  layers.Dense(1)\n","  ])\n","\n","\n","model_2 = Sequential([\n","  layers.Conv1D(128, kernel_size = kernel_size, padding = \"same\", activation=\"relu\", input_shape = (window_size,5)),\n","  layers.MaxPooling1D(pool_size= pool_size, padding='same'),\n","  layers.Conv1D(128,kernel_size = kernel_size, padding = \"same\", activation=\"relu\"),\n","  layers.MaxPooling1D(pool_size= pool_size, padding='same'),\n","  layers.GlobalAveragePooling1D(),\n","  # layers.Dropout(0.5),\n","  layers.Dense(96, activation = \"relu\"),\n","  layers.Dense(1)\n","  ])\n","\n","model_3 = Sequential([\n","  layers.Conv1D(128, kernel_size = kernel_size, padding = \"same\", activation=\"relu\", input_shape = (window_size,5)),\n","  layers.MaxPooling1D(pool_size= pool_size, padding='same'),\n","  layers.Conv1D(128,kernel_size = kernel_size, padding = \"same\", activation=\"relu\"),\n","  layers.MaxPooling1D(pool_size= pool_size, padding='same'),\n","  layers.GlobalAveragePooling1D(),\n","  # layers.Dropout(0.5),\n","  layers.Dense(96, activation = \"relu\"),\n","  layers.Dense(1)\n","  ])\n","\n","model_4 = Sequential([\n","  layers.Conv1D(128, kernel_size = kernel_size, padding = \"same\", activation=\"relu\", input_shape = (window_size,5)),\n","  layers.MaxPooling1D(pool_size= pool_size, padding='same'),\n","  layers.Conv1D(128,kernel_size = kernel_size, padding = \"same\", activation=\"relu\"),\n","  layers.MaxPooling1D(pool_size= pool_size, padding='same'),\n","  layers.GlobalAveragePooling1D(),\n","  # layers.Dropout(0.5),\n","  layers.Dense(96, activation = \"relu\"),\n","  layers.Dense(1)\n","  ])\n","\n","model_5 = Sequential([\n","  layers.Conv1D(128, kernel_size = kernel_size, padding = \"same\", activation=\"relu\", input_shape = (window_size,5)),\n","  layers.MaxPooling1D(pool_size= pool_size, padding='same'),\n","  layers.Conv1D(128,kernel_size = kernel_size, padding = \"same\", activation=\"relu\"),\n","  layers.MaxPooling1D(pool_size= pool_size, padding='same'),\n","  layers.GlobalAveragePooling1D(),\n","  # layers.Dropout(0.5),\n","  layers.Dense(96, activation = \"relu\"),\n","  layers.Dense(1)\n","  ])\n","\n","\n","# def scheduler(epoch):\n","#   if epoch < 10:\n","#     return 0.001\n","#   else:\n","#     return 0.0001\n","# callback = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose = 0)\n","\n","model_1.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse') # tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","model_2.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n","model_3.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n","model_4.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n","model_5.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mse')\n","\n","model_1.fit(train_output_data_1, train_output_targets_1 , epochs=epoch, batch_size= batch_size, verbose=0)\n","model_2.fit(train_output_data_2, train_output_targets_2 , epochs=epoch, batch_size= batch_size, verbose=0)\n","model_3.fit(train_output_data_3, train_output_targets_3 , epochs=epoch, batch_size= batch_size, verbose=0)\n","model_4.fit(train_output_data_4, train_output_targets_4 , epochs=epoch, batch_size= batch_size, verbose=0)\n","model_5.fit(train_output_data_5, train_output_targets_5 , epochs=epoch, batch_size= batch_size, verbose=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14642,"status":"ok","timestamp":1700588704449,"user":{"displayName":"蔡承哲","userId":"08856972309667959252"},"user_tz":-480},"id":"ExWHcHMalQgw","outputId":"63c4e267-420b-4cac-df34-4db62151e1d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting optuna\n","  Downloading optuna-3.4.0-py3-none-any.whl (409 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.6/409.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.12.1-py3-none-any.whl (226 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting colorlog (from optuna)\n","  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.23)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.1)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n","Installing collected packages: Mako, colorlog, alembic, optuna\n","Successfully installed Mako-1.3.0 alembic-1.12.1 colorlog-6.7.0 optuna-3.4.0\n","Collecting cmaes\n","  Downloading cmaes-0.10.0-py3-none-any.whl (29 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from cmaes) (1.23.5)\n","Installing collected packages: cmaes\n","Successfully installed cmaes-0.10.0\n"]}],"source":["!pip install optuna\n","!pip install cmaes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JA0AKeXYPujC"},"outputs":[],"source":["import optuna\n","from functools import partial\n","\n","def _objective(trial, y_true, y_preds):\n","    weights = [trial.suggest_float(f\"weight{n}\", 1e-15, 1) for n in range(len(y_preds))]  # 定義每個預測模型的權重\n","    weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)  # 產生加權的預測值\n","    score = np.sqrt(mean_squared_error(y_true, weighted_pred))  # 計算加權預測的AUC Score\n","    return score\n","\n","def OptunaWeights(y_true, y_preds):\n","    # 最佳化的過程不顯示每個trial的狀態\n","    # optuna.logging.set_verbosity(optuna.logging.ERROR)\n","    # 使用 CMA-ES取樣器的Optuna取樣器實例，CMA-ES是一種進化策略演算法，通常用於連續參數的優化問題\n","    sampler = optuna.samplers.CmaEsSampler(seed=42)\n","    # 使用 Hyperband 演算法的 Optuna 修剪器，Hyperband 是一種基於競爭性的修剪方法，可以在較少的試驗次數下找到有潛力的參數組合，從而節省時間和資源。\n","    pruner = optuna.pruners.HyperbandPruner()\n","    study = optuna.create_study(sampler=sampler, pruner=pruner, study_name=\"OptunaWeights\", direction='minimize')\n","    objective_partial = partial(_objective, y_true = y_true , y_preds=y_preds)\n","    study.optimize(objective_partial, n_trials=200)\n","    # 將找到的最佳權重儲存在 weights\n","    weights = [study.best_params[f\"weight{n}\"] for n in range(len(y_preds))]\n","    return weights\n","\n","def weighted_predict(y_preds,weights):\n","    weighted_pred = np.average(np.array(y_preds).T, axis=1, weights=weights)\n","    return weighted_pred"]},{"cell_type":"markdown","metadata":{"id":"4cyFxzImtSDg"},"source":["### 最佳化權重"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3926,"status":"ok","timestamp":1700470350983,"user":{"displayName":"蔡承哲","userId":"08856972309667959252"},"user_tz":-480},"id":"rtMvr4WKkkeD","outputId":"09f3c148-4b68-4c83-cad3-0cb84e7ba0cd"},"outputs":[{"name":"stdout","output_type":"stream","text":["214/214 [==============================] - 0s 2ms/step\n","214/214 [==============================] - 0s 2ms/step\n","214/214 [==============================] - 0s 2ms/step\n","214/214 [==============================] - 0s 2ms/step\n","214/214 [==============================] - 0s 2ms/step\n"]}],"source":["# data 1\n","test_x = A1.drop(['y1','y2','y3'],axis= 1)\n","y_val = A1['y1'][window_size-1:]\n","\n","# 正規化數據\n","scaled_test_data = MinMaxScaler().fit_transform(test_x)\n","test_output_data = process_input_data_with_targets( scaled_test_data,None,window_length= window_size, shift = 1)\n","prediction_2 = model_2.predict(test_output_data).reshape(-1)\n","prediction_3 = model_3.predict(test_output_data).reshape(-1)\n","prediction_4 = model_4.predict(test_output_data).reshape(-1)\n","prediction_5 = model_5.predict(test_output_data).reshape(-1)\n","\n","A1_y1_preds = np.array([prediction_2, prediction_3 , prediction_4 ,prediction_5,prediction_6])\n","A1_weight = OptunaWeights(y_val.values, A1_y1_preds) # 利用每個模型的驗證資料預測值實際值，找出最佳權重\n","# y_val_pred = weighted_predict(A1_y1_preds,A1_weight) # 加權驗證資料的預測值，得出加權預測值\n","\n","# A2 data\n","test_x = A2.drop(['y1','y2','y3'],axis= 1)\n","y_val = A2['y1'][window_size-1:]\n","\n","prediction_1 = model_1.predict(test_output_data).reshape(-1)\n","prediction_3 = model_3.predict(test_output_data).reshape(-1)\n","prediction_4 = model_4.predict(test_output_data).reshape(-1)\n","prediction_5 = model_5.predict(test_output_data).reshape(-1)\n","\n","A2_y1_preds = np.array([prediction_1, prediction_3 , prediction_4 ,prediction_5,prediction_6])\n","A2_weight = OptunaWeights(y_val.values, A2_y1_preds) # 利用每個模型的驗證資料預測值實際值，找出最佳權重\n","\n","# A3 data\n","\n","test_x = A3.drop(['y1','y2','y3'],axis= 1)\n","y_val = A3['y1'][window_size-1:]\n","\n","prediction_1 = model_1.predict(test_output_data).reshape(-1)\n","prediction_2 = model_2.predict(test_output_data).reshape(-1)\n","prediction_3 = model_3.predict(test_output_data).reshape(-1)\n","prediction_4 = model_4.predict(test_output_data).reshape(-1)\n","prediction_5 = model_5.predict(test_output_data).reshape(-1)\n","\n","A3_y1_preds = np.array([prediction_1, prediction_2 , prediction_4 ,prediction_5, prediction_6])\n","A3_weight = OptunaWeights(y_val.values, A3_y1_preds) # 利用每個模型的驗證資料預測值實際值，找出最佳權重\n","\n","# A4 data\n","\n","test_x = A4.drop(['y1','y2','y3'],axis= 1)\n","y_val = A4['y1'][window_size-1:]\n","\n","prediction_1 = model_1.predict(test_output_data).reshape(-1)\n","prediction_2 = model_2.predict(test_output_data).reshape(-1)\n","prediction_3 = model_3.predict(test_output_data).reshape(-1)\n","prediction_4 = model_4.predict(test_output_data).reshape(-1)\n","prediction_5 = model_5.predict(test_output_data).reshape(-1)\n","\n","A4_y1_preds = np.array([prediction_1, prediction_2 , prediction_3 ,prediction_5, prediction_6])\n","A4_weight = OptunaWeights(y_val.values, A4_y1_preds) # 利用每個模型的驗證資料預測值實際值，找出最佳權重\n","\n","# A5 data\n","test_x = A5.drop(['y1','y2','y3'],axis= 1)\n","y_val = A5['y1'][window_size-1:]\n","\n","prediction_1 = model_1.predict(test_output_data).reshape(-1)\n","prediction_2 = model_2.predict(test_output_data).reshape(-1)\n","prediction_3 = model_3.predict(test_output_data).reshape(-1)\n","prediction_4 = model_4.predict(test_output_data).reshape(-1)\n","prediction_5 = model_5.predict(test_output_data).reshape(-1)\n","\n","A5_y1_preds = np.array([prediction_1, prediction_2 , prediction_3 ,prediction_5, prediction_6])\n","A5_weight = OptunaWeights(y_val.values, A5_y1_preds) # 利用每個模型的驗證資料預測值實際值，找出最佳權重"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MYyYG5sz145g"},"outputs":[],"source":["print(A1_weight,A2_weight,A3_weight,A4_weight,A5_weight)"]},{"cell_type":"markdown","metadata":{},"source":["final weight  \n","[0.0010165189829187068, 0.9368349643618032, 0.0005747298156939793, 0.0010327902810488498]"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyO1t8IWon8Mp4i43vXYjJwS","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
